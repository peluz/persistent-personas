{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922ce24-b7b0-46c1-be54-5c0c23ecd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import statsmodels.formula.api as smf\n",
    "from data.loader import load_data\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900cbd5-2bc5-4543-b5d2-b706e33f06a0",
   "metadata": {},
   "source": [
    "### Matplotlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08acea5b-86a1-46a1-91d2-0f75f9f2d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.fontManager.addfont(\"/usr/share/fonts/truetype/cmu/cmunrm.ttf\")   # regular\n",
    "fm.fontManager.addfont(\"/usr/share/fonts/truetype/cmu/cmunbx.ttf\")   # bold\n",
    "bold_font = fm.FontProperties(fname=\"/usr/share/fonts/truetype/cmu/cmunbx.ttf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf25bd9-2199-4f31-a99d-82bc667a21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_serif = fm.FontProperties(fname=\"/usr/share/fonts/truetype/cmu/cmunrm.ttf\").get_name()\n",
    "print(\"Font name:\", cmu_serif)  # should be \"CMU Serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e19c6e-ce84-42d8-b4fc-10bf6bc9b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,  # Enable LaTeX\n",
    "    \"mathtext.fontset\": \"cm\",  # Use Computer Modern (LaTeX default)\n",
    "    \"font.family\": cmu_serif,\n",
    "    \"font.size\": 16,         # Base font size\n",
    "    \"axes.titlesize\": 18,    # Title font size\n",
    "    \"axes.labelsize\": 16,    # Axis label font size\n",
    "    \"xtick.labelsize\": 14,   # X-axis tick font size\n",
    "    \"ytick.labelsize\": 14,   # Y-axis tick font size\n",
    "    \"legend.fontsize\": 16    # Legend font size\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1ecdf-4409-4cc5-b783-1ce6cd634be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8b0fb-77be-4dd1-afa4-81f61aadf419",
   "metadata": {},
   "source": [
    "### Dialogue eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e06e3-d3cc-4d68-8a42-e4020ff85bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./results/interview_ratings.csv\")\n",
    "ratings_original = ratings[~ratings.role.str.contains(\"shuffle\")].copy()\n",
    "ratings_shuffled = ratings[ratings.role.str.contains(\"shuffle\")].copy()\n",
    "ratings_original[\"Selene-1-Mini-Llama-3.1-8B\"] = (ratings_original[\"Selene-1-Mini-Llama-3.1-8B\"] + ratings_shuffled[\"Selene-1-Mini-Llama-3.1-8B\"].values)/2\n",
    "persona_directed_all = ratings_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e68f9-77af-4d99-82c6-0d270be21edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./results/instructions_ratings.csv\")\n",
    "ratings_original = ratings[~ratings.role.str.contains(\"shuffle\")].copy()\n",
    "ratings_shuffled = ratings[ratings.role.str.contains(\"shuffle\")].copy()\n",
    "ratings_original[\"Selene-1-Mini-Llama-3.1-8B\"] = (ratings_original[\"Selene-1-Mini-Llama-3.1-8B\"] + ratings_shuffled[\"Selene-1-Mini-Llama-3.1-8B\"].values)/2\n",
    "goal_oriented_all = ratings_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84ff18-5f20-4325-a632-968805ebb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_directed = persona_directed_all.groupby(['metric', 'role', 'idx']).mean(numeric_only=True).reset_index()\n",
    "goal_oriented = goal_oriented_all.groupby(['metric', 'role', 'idx']).mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3083ab-5616-4e46-9f83-58ca8ef9f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your fixed indices\n",
    "fixed_indices = [0, 11, 22, 34, 45, 56, 68, 79, 90, 101]\n",
    "\n",
    "def smooth_segmented(x, y, indices):\n",
    "    x_smooth, y_smooth = [], []\n",
    "    for i, idx in enumerate(indices):\n",
    "        if i == 0:\n",
    "            # First point: take exact y at x == 0\n",
    "            if idx in x:\n",
    "                y_val = y[x == idx].mean()  # in case multiple values at x=0\n",
    "            else:\n",
    "                y_val = np.nan\n",
    "            x_smooth.append(idx)\n",
    "            y_smooth.append(y_val)\n",
    "        else:\n",
    "            start = indices[i-1] + 1\n",
    "            end = idx\n",
    "            mask = (x >= start) & (x <= end)\n",
    "            if mask.sum() > 0:\n",
    "                x_smooth.append(idx)\n",
    "                y_smooth.append(y[mask].mean())\n",
    "    return np.array(x_smooth), np.array(y_smooth)\n",
    "\n",
    "# Create a single figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Group data by metric\n",
    "metric_groups = (\n",
    "    persona_directed[~persona_directed.role.str.contains(\"empty\")]\n",
    "    .groupby(['metric', 'idx'])['Selene-1-Mini-Llama-3.1-8B']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .groupby(\"metric\")\n",
    ")\n",
    "\n",
    "# Plotting loop\n",
    "for i, (metric, group_df) in enumerate(metric_groups):\n",
    "    ax = axes[i]\n",
    "    ax.set_title(metric.capitalize().replace(\"_\", \"-\") + \" (↑)\", fontproperties=bold_font, fontsize=16)\n",
    "    ax.set_xlabel('Conversation round')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Persona-directed (persona)\n",
    "    x = group_df['idx'].values\n",
    "    y = group_df['Selene-1-Mini-Llama-3.1-8B'].values\n",
    "    x_smooth, y_smooth = smooth_segmented(x, y, fixed_indices)\n",
    "    ax.plot(x, y, marker='o', color=\"orange\", alpha=0.3, label='_nolegend_')\n",
    "    ax.plot(x_smooth, y_smooth, marker='o', color=\"orange\", linestyle='-', label='Persona-directed (persona)')\n",
    "\n",
    "    # Goal-oriented (persona)\n",
    "    group_df2 = (\n",
    "        goal_oriented[~goal_oriented.role.str.contains(\"empty\")]\n",
    "        [goal_oriented.metric == metric]\n",
    "        .groupby(\"idx\")[\"Selene-1-Mini-Llama-3.1-8B\"].mean()\n",
    "    )\n",
    "    x = group_df2.index.values\n",
    "    y = group_df2.values\n",
    "    x_smooth, y_smooth = smooth_segmented(x, y, fixed_indices)\n",
    "    ax.plot(x, y, marker='x', color=\"blue\", alpha=0.3, label='_nolegend_')\n",
    "    ax.plot(x_smooth, y_smooth, marker='x', color=\"blue\", linestyle='-', label='Goal-oriented (persona)')\n",
    "    \n",
    "    # Persona-directed (baseline)\n",
    "    empty_results = (\n",
    "        persona_directed[persona_directed.metric == metric]\n",
    "        [persona_directed.role.str.contains(\"empty\")]\n",
    "        .groupby(\"idx\")[\"Selene-1-Mini-Llama-3.1-8B\"].mean()\n",
    "    )\n",
    "    x = empty_results.index.values\n",
    "    y = empty_results.values\n",
    "    x_smooth, y_smooth = smooth_segmented(x, y, fixed_indices)\n",
    "    ax.plot(x, y, marker='o', color=\"orange\", alpha=0.3, linestyle='--',label='_nolegend_')\n",
    "    ax.plot(x_smooth, y_smooth, marker='o', color=\"orange\", linestyle='--', label='Persona-directed (baseline)')\n",
    "\n",
    "\n",
    "    # Goal-oriented (baseline)\n",
    "    empty_results2 = (\n",
    "        goal_oriented[goal_oriented.metric == metric]\n",
    "        [goal_oriented.role.str.contains(\"empty\")]\n",
    "        .groupby(\"idx\")[\"Selene-1-Mini-Llama-3.1-8B\"].mean()\n",
    "    )\n",
    "    x = empty_results2.index.values\n",
    "    y = empty_results2.values\n",
    "    x_smooth, y_smooth = smooth_segmented(x, y, fixed_indices)\n",
    "    ax.plot(x, y, marker='x', color=\"blue\", alpha=0.3,linestyle='--', label='_nolegend_')\n",
    "    ax.plot(x_smooth, y_smooth, marker='x', color=\"blue\", linestyle='--', label='Goal-oriented (baseline)')\n",
    "    #ax.axvline(14)\n",
    "\n",
    "# Set common y-axis label\n",
    "axes[0].set_ylabel('Rating')\n",
    "\n",
    "# Create a single legend for the entire figure\n",
    "lines, labels = [], []\n",
    "for ax in axes:\n",
    "    line, label = ax.get_legend_handles_labels()\n",
    "    lines.extend(line)\n",
    "    labels.extend(label)\n",
    "unique_labels = dict(zip(labels, lines))\n",
    "fig.legend(unique_labels.values(), unique_labels.keys(), loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=4)\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae3f80-057e-4eb7-bb27-cce51e7de50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure to PDF\n",
    "fig.savefig('../persistent-personas-paper/media/dialogue_metrics.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429d381-ed53-4edd-a7a9-3327de4e45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_directed = persona_directed_all.rename(columns=lambda x: \"persona\" if x == \"Selene-1-Mini-Llama-3.1-8B\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b37337-efa4-45eb-9d8c-b9572c52ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_directed[\"goal\"] = goal_oriented_all[\"Selene-1-Mini-Llama-3.1-8B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a5bfc-3d17-4971-8a69-7edda0beda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_directed.role = persona_directed.role.apply(lambda x: x.replace(\"interview_\", \"\").replace(\"instructions_\", \"\").replace(\".json.csv\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829525a5-be10-43a7-a8d9-9397f7cc0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings[\"dialogue\"] = persona_directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4761c-db5c-41f8-8522-e674681f3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_directed[\"diff\"] = persona_directed[\"persona\"] - persona_directed[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c51d2-c378-482f-b643-2f6328462446",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_directed = persona_directed[~persona_directed.role.str.contains(\"empty\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60d129-225b-428f-a667-27969bdde0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f84520-49ea-47b0-8cb5-dd0c181c698e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = persona_directed[persona_directed.metric==\"style\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['diff']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] >0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa980048-2d56-464e-b391-42ed86cf2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. style difference (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/style_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a256e1b-74da-4739-a488-2a8f7ca8dc4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = persona_directed[persona_directed.metric==\"knowledge\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['diff']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] >0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cf5fd-09f1-4147-b8c7-91f79035c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. knowledge difference (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/knowledge_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885f0ee-cdd5-442d-b0cb-1e25849b57d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = persona_directed[persona_directed.metric==\"in_character\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['diff']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] >0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff2550-69b3-44d4-a6ae-29ec346935fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. in-character difference (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/in_character_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b19d9-63f6-4fd0-ba1f-fa2bba27db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = persona_directed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09f714-acdd-40ce-8896-d006c2da472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = df[df['idx'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"metric\", \"model\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['persona'] - df_merged['persona_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['goal'] - df_merged['goal_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671361d-07e6-473e-94c5-50844151df2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.metric==\"style\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9679c-5f73-41ed-b265-93a131ea595c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.metric==\"style\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241d29f-8a2f-433b-bfc6-15f7ea43fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. style progression', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/style_dialogue_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f2ce4-6214-4d82-ba58-56e3cca1acd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.metric==\"knowledge\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45242b13-5b91-480a-97b9-2918cb7863c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.metric==\"knowledge\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb15ce4-d7c5-4229-8a28-22da5e7786d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. knowledge progression', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/knowledge_dialogue_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd53a5-38e7-4998-ad59-be00b8926a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.metric==\"in_character\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ccf6f-4c66-4cf5-891f-3fe3cdb4fd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.metric==\"in_character\"].groupby('idx')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db1c77-b71d-4e12-820e-46981837e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. in-character progression', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/in_character_dialogue_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac5072-0749-4062-934b-fc3fc2eded0d",
   "metadata": {},
   "source": [
    "# BFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825698e-d7ca-4377-9fd6-b364982a3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./results/bfi_ratings.csv\")\n",
    "\n",
    "labels = json.load(open(\"./data/characters_labels.json\", \"r\"))\n",
    "\n",
    "bfi = json.load(open(\"./data/BFI.json\", \"r\"))\n",
    "\n",
    "dimensions =[x[\"dimension\"] for x in  bfi[\"questions\"].values()]\n",
    "\n",
    "ratings[\"dimension\"]= ratings.idx.apply(lambda x: dimensions[x])\n",
    "\n",
    "ratings[\"base_conversation\"] = ratings.role.apply(lambda x: \"instructions\" if \"inst-\" in x else \"interview\")\n",
    "\n",
    "ratings[\"pos\"] = ratings.role.str[:-9].str.split(\"_\").str[-1].astype(int)\n",
    "\n",
    "ratings\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.rsplit(\"_\", n=1).str[0]\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.split(\"_\",n=1).str[-1].str.replace(\"_\", \" \")\n",
    "\n",
    "ratings = ratings.groupby(['model', 'role', 'pos', \"base_conversation\", 'dimension'])['rating'].mean().unstack(fill_value=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe03ec2-9f1c-4efa-acff-a52230b7caa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings[\"MAE_empty\"] = ratings.apply(lambda x: mean_absolute_error(x[dimensions],\n",
    "                                          ratings[ratings.model==x[\"model\"]][ratings.role==\"empty\"][ratings.pos==x[\"pos\"]][ratings.base_conversation == x[\"base_conversation\"]].iloc[0][dimensions])/4,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb528e-a83a-4096-9fae-77b453b1cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_avg = ratings.groupby(['role', 'pos', 'base_conversation']).mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1bd163-8db4-4979-af0e-2e053a994e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_ratings = ratings[(ratings.role.isin(labels.keys())) | (ratings.role.str.contains(\"empty\"))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28840894-0bd9-42af-864f-7b50012221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"Agreeableness\", \"Conscientiousness\", \"Extraversion\", \"Neuroticism\", \"Openness\"]\n",
    "labelled_ratings.loc[~labelled_ratings.role.str.contains(\"empty\"),\"MAE\"] = labelled_ratings[~labelled_ratings.role.str.contains(\"empty\")].apply(lambda x: mean_absolute_error(x[dimensions],\n",
    "                                          [labels[x[\"role\"]][\"BFI\"][dim][\"score\"] for dim in dimensions])/4,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_ratings.loc[labelled_ratings.role.str.contains(\"empty\"),\"MAE\"] = labelled_ratings[labelled_ratings.role.str.contains(\"empty\")].apply(lambda x: {role: mean_absolute_error(x[dimensions],\n",
    "                                          [labels[role][\"BFI\"][dim][\"score\"] for dim in dimensions])/4 for role in labels.keys()},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b927f-b219-4961-97a7-ec889d5b1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings[\"bfi\"] = labelled_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abd389-90aa-4623-b22d-1c0771fd5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# --- First Plot (Top Subplot) ---\n",
    "ax1 = axes[0]\n",
    "df = labelled_ratings[\n",
    "    (labelled_ratings.base_conversation == \"interview\") &\n",
    "    (~labelled_ratings.role.str.contains(\"empty\"))\n",
    "].groupby('pos')[\"MAE\"].mean().reset_index()\n",
    "ax1.plot(df['pos'], df[\"MAE\"], color=\"orange\", marker='o', label=\"Persona-directed\")\n",
    "df = labelled_ratings[\n",
    "    (labelled_ratings.base_conversation == \"interview\") &\n",
    "    (labelled_ratings.role.str.contains(\"empty\"))\n",
    "]\n",
    "df[\"MAE\"] = df.MAE.apply(lambda x: np.mean(list(x.values())))\n",
    "df = df.groupby('pos')[\"MAE\"].mean().reset_index()\n",
    "display(df)\n",
    "ax1.plot(df['pos'], df[\"MAE\"], color=\"orange\", marker='o', label=\"Persona-directed (baseline)\", linestyle='--')\n",
    "\n",
    "df = labelled_ratings[\n",
    "    (labelled_ratings.base_conversation == \"instructions\") &\n",
    "    (~labelled_ratings.role.str.contains(\"empty\"))\n",
    "].groupby('pos')[\"MAE\"].mean().reset_index()\n",
    "ax1.plot(df['pos'], df[\"MAE\"], color=\"blue\", marker='x', label=\"Goal-oriented\")\n",
    "df = labelled_ratings[\n",
    "    (labelled_ratings.base_conversation == \"instructions\") &\n",
    "    (labelled_ratings.role.str.contains(\"empty\"))\n",
    "]\n",
    "df[\"MAE\"] = df.MAE.apply(lambda x: np.mean(list(x.values())))\n",
    "df = df.groupby('pos')[\"MAE\"].mean().reset_index()\n",
    "ax1.plot(df['pos'], df[\"MAE\"], color=\"blue\", marker='x', label=\"Goal-oriented (baseline)\", linestyle='--')\n",
    "\n",
    "ax1.set_title(f'Big Five Traits: Roles vs. Ground Truth (↓)', fontproperties=bold_font, fontsize=14)\n",
    "ax1.set_ylabel('')\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Second Plot (Bottom Subplot) ---\n",
    "ax2 = axes[1]\n",
    "df = ratings[\n",
    "    (ratings.base_conversation == \"interview\") &\n",
    "    (~ratings.role.str.contains(\"empty\"))\n",
    "].groupby('pos')[\"MAE_empty\"].mean().reset_index()\n",
    "ax2.plot(df['pos'], df[\"MAE_empty\"], marker='o', color=\"orange\", label=\"Persona-directed\")\n",
    "\n",
    "df = ratings[\n",
    "    (ratings.base_conversation == \"instructions\") &\n",
    "    (~ratings.role.str.contains(\"empty\"))\n",
    "].groupby('pos')[\"MAE_empty\"].mean().reset_index()\n",
    "ax2.plot(df['pos'], df[\"MAE_empty\"], marker='x', color=\"blue\", label=\"Goal-oriented\")\n",
    "\n",
    "ax2.set_title(f'Big Five Traits: Roles vs. Baseline', fontproperties=bold_font, fontsize=14)\n",
    "ax2.set_xlabel('Conversation round')\n",
    "ax2.set_ylabel('')\n",
    "ax2.grid(True)\n",
    "\n",
    "fig.text(0.01, 0.5, 'Normalized mean absolute difference', va='center', rotation='vertical')\n",
    "\n",
    "# --- Combined Legend and Layout ---\n",
    "# Get handles and labels from one of the subplots (they are the same)\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "# Place the legend at the top of the entire figure\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.55, 1.09), ncol=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb93d59-7207-468c-8c8f-db2fc24ed696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/bfi_mae.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736d49b-2599-48fc-a47e-81edda326606",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = labelled_ratings[labelled_ratings.role != \"empty\"].pivot_table(index=['model', 'role', 'pos'],\n",
    "                            columns='base_conversation',\n",
    "                            values='MAE').reset_index()\n",
    "\n",
    "# Calculate the difference (MAE_interview - MAE_instructions)\n",
    "pivoted_df['MAE_difference'] = pivoted_df['interview'] - pivoted_df['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887710e4-995c-41b5-b467-29a564f3b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = pivoted_df.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['MAE_difference']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results, index=np.unique(pivoted_df.pos))\n",
    "df= results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53efc9ae-4680-4ce2-b935-13d1d3796782",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Mean Absolute difference from Ground Truth (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/mae_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c56cb8-b364-4be9-9f05-6865b18e3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = ratings[ratings.role != \"empty\"].pivot_table(index=['model', 'role', 'pos'],\n",
    "                            columns='base_conversation',\n",
    "                            values='MAE_empty').reset_index()\n",
    "\n",
    "# Calculate the difference (MAE_interview - MAE_instructions)\n",
    "pivoted_df['MAE_difference'] = pivoted_df['interview'] - pivoted_df['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163adfe4-89a1-46f5-a1f1-bf6816500b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = pivoted_df.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['MAE_difference']\n",
    "\n",
    "    # Skip groups with too few samples to perform meaningful bootstrapping\n",
    "    if len(diffs) < 2:\n",
    "        print(f\"Skipping 'pos' {pos} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results, index=np.unique(pivoted_df.pos))\n",
    "\n",
    "df = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d048a-2dd4-471c-8722-5e68753dd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Mean Absolute Difference from Baseline (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/mae_empty_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7f3bd-a1d7-4266-98bb-96dff8498e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = labelled_ratings[labelled_ratings.role != \"empty\"].pivot_table(index=['model', 'role', 'pos'],\n",
    "                            columns='base_conversation',\n",
    "                            values='MAE').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f25ce-26b9-4746-a20b-9e7fd231f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pivoted_df.copy()\n",
    "baseline_df = df[df['pos'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6d6cf-22ec-4d13-ac3e-d216985457d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c64106-b21f-46c2-956d-e32f39730639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86457d56-ffc0-40d3-9c87-3e782141f8e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Mean Absolute Difference from Ground Truth progression', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/mae_ground_truth_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e2324-d71c-4855-81ff-731ffca84b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = ratings[ratings.role != \"empty\"].pivot_table(index=['model', 'role', 'pos'],\n",
    "                            columns='base_conversation',\n",
    "                            values='MAE_empty').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a579df-310c-4275-adc6-097c33f9688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pivoted_df.copy()\n",
    "baseline_df = df[df['pos'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ad24c-e038-42fd-9bd3-4bd257589c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaea8f-b986-4635-a4e5-b254fd50744c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44dc0f-d914-43b4-a389-c2270dc24b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Mean Absolute Difference from Baseline progression', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/mae_empty_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd86869-7acd-4b04-93b9-3c42689d3a49",
   "metadata": {},
   "source": [
    "# Instruction persona-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144879c4-d29b-4ecd-bf0a-6893bc647147",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./results/instruction_role_specific_ratings.csv\")\n",
    "\n",
    "data, _ = load_data(\"instruction_role_specific\")\n",
    "\n",
    "ratings[ratings.role.str.contains(\"previous\")].role.tolist()[0]\n",
    "\n",
    "ratings[\"reference\"] = \"dataset\"\n",
    "ratings.loc[ratings.role.str.contains(\"_zero_reference\"), \"reference\"] = \"0_answer\"\n",
    "ratings.loc[ratings.role.str.contains(\"_previous_reference\"), \"reference\"] = \"previous_answer\"\n",
    "ratings[\"role\"] = ratings[\"role\"].apply(lambda x: x.replace(\"_zero_reference\", \"\"))\n",
    "ratings[\"role\"] = ratings[\"role\"].apply(lambda x: x.replace(\"_previous_reference\", \"\"))\n",
    "\n",
    "even_mapping = {\"A\": 0, \"B\": 1}\n",
    "odd_mapping = {\"A\": 1, \"B\": 0}\n",
    "ratings[\"rating\"] = ratings.apply(lambda x: even_mapping[x[\"rating\"]] if x[\"idx\"]%2==0 else odd_mapping[x[\"rating\"]], axis=1)\n",
    "\n",
    "ratings[\"pos\"] = ratings.role.str.extract(r\"(\\d+)(?=\\.json)\").astype(int)\n",
    "\n",
    "ratings[\"base_conversation\"] = ratings.role.apply(lambda x: \"instructions\" if \"inst-\" in x else \"interview\")\n",
    "\n",
    "ratings[\"role\"] = ratings.role.str.replace(r\"empty_\\d+.json\", \"empty\", regex=True)\n",
    "\n",
    "ratings[\"role\"] = ratings.role.apply(lambda x: x.replace(\".csv\", \"_.csv\") if \"empty\" in x else x)\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.rsplit(\"_\", n=1).str[0]\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.split(\"_\",n=3).str[-1].str.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a07c5-87d4-4316-b2cd-ef630a1a2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.groupby(['model', 'role', 'pos', \"reference\", \"base_conversation\"])['rating'].mean().unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4084950-282e-4e83-b22c-74baed372521",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings[\"role_specific\"] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc531abf-f4bb-43ed-9ae3-8e2f3cf5bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings[(ratings.reference == \"dataset\")  &(~ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "ax.plot(df.pos, df.interview, \n",
    "             color=\"orange\", marker='o', label='Persona-directed')\n",
    "\n",
    "# Plot horizontal line\n",
    "#plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "\n",
    "ax.plot(df.pos, df.instructions, \n",
    "             color=\"blue\", marker='x', label='Goal-oriented')\n",
    "\n",
    "\n",
    "ax.set_title('Quality of responses to persona-specific instructions (↑)', fontproperties=bold_font, fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('Conversation round')\n",
    "ax.set_ylabel('Win rate')\n",
    "ax.grid(True)\n",
    "#plt.ylim(0, 1)\n",
    "# # Sanitize role name for filename\n",
    "# sanitized_role = role.replace('.json.csv', '').replace('.', '_').replace('/', '_')\n",
    "# filename = f'line_plot_{metric}_{sanitized_role}.png'\n",
    "# plt.savefig(filename)\n",
    "# plt.close() # Close the plot to free memory\n",
    "# print(f\"Generated plot: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69095ecd-6ad8-4fe3-a8aa-6cd33f77ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/role_specific.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309befe-a6b6-4d99-8dc2-03599591b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = ratings[~ratings.role.str.contains(\"empty\")][ratings.reference==\"dataset\"]\n",
    "# Calculate the difference (MAE_interview - MAE_instructions)\n",
    "filtered_df['rating_difference'] = filtered_df['interview'] - filtered_df['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06610a31-e5b0-40f5-a8e9-956870108be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = filtered_df.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['rating_difference']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "df = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e39370-84fe-4d5a-8fda-9836db1653b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. persona-specific response quality difference (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/role_specific_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890605f-39f9-4eb9-bb79-3eb62694a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()\n",
    "baseline_df = df[df['pos'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac907a2c-849a-4dd0-9c94-6ba5d35f6681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f2244-19fe-4ccf-aed6-d66d761e5cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85588e65-70da-4e6b-ac9f-188a15f0ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. persona-specific response quality progression', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/role_specific_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b2b3a-7cec-4038-9264-f911b534fa53",
   "metadata": {},
   "source": [
    "# Instruction general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efd8ac-039a-490e-9c0f-5917f3ec383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./results/instruction_general_ratings.csv\")\n",
    "\n",
    "data, _ = load_data(\"instruction_general\")\n",
    "\n",
    "ratings[\"reference\"] = \"dataset\"\n",
    "ratings.loc[ratings.role.str.contains(\"_zero_reference\"), \"reference\"] = \"0_answer\"\n",
    "ratings.loc[ratings.role.str.contains(\"_previous_reference\"), \"reference\"] = \"previous_answer\"\n",
    "ratings[\"role\"] = ratings[\"role\"].apply(lambda x: x.replace(\"_zero_reference\", \"\"))\n",
    "ratings[\"role\"] = ratings[\"role\"].apply(lambda x: x.replace(\"_previous_reference\", \"\"))\n",
    "\n",
    "even_mapping = {\"A\": 0, \"B\": 1}\n",
    "odd_mapping = {\"A\": 1, \"B\": 0}\n",
    "ratings[\"rating\"] = ratings.apply(lambda x: even_mapping[x[\"rating\"]] if x[\"idx\"]%2==0 else odd_mapping[x[\"rating\"]], axis=1)\n",
    "\n",
    "ratings[\"pos\"] = ratings.role.str.extract(r\"(\\d+)(?=\\.json)\").astype(int)\n",
    "\n",
    "ratings[\"base_conversation\"] = ratings.role.apply(lambda x: \"instructions\" if \"inst-\" in x else \"interview\")\n",
    "\n",
    "ratings[\"role\"] = ratings.role.str.replace(r\"empty_\\d+.json\", \"empty\", regex=True)\n",
    "\n",
    "ratings[\"role\"] = ratings.role.apply(lambda x: x.replace(\".csv\", \"_.csv\") if \"empty\" in x else x)\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.rsplit(\"_\", n=1).str[0]\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.split(\"_\",n=2).str[-1].str.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785865f-1349-4dc3-9d5d-aaf29ce8f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.groupby(['model', 'role', 'pos', \"reference\", \"base_conversation\"])['rating'].mean().unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0c783-bc2d-43fc-a9ae-4caca96a95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings[\"general\"] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f37458-8c1d-46ee-8552-31a6a1107e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "df = ratings[(ratings.reference == \"dataset\")   & (ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "ax.plot(df.pos, df.interview, \n",
    "             color=\"orange\", marker=\"o\", linestyle=\"--\", label='Persona-directed (baseline)')\n",
    "\n",
    "# Plot horizontal line\n",
    "#plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "\n",
    "ax.plot(df.pos, df.instructions, \n",
    "             color=\"blue\", marker='x',linestyle=\"--\", label='Goal-oriented (baseline)')\n",
    "\n",
    "df = ratings[(ratings.reference == \"dataset\")  & (~ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "ax.plot(df.pos, df.interview, \n",
    "             color=\"orange\", marker='o', label='Persona-directed (persona)')\n",
    "\n",
    "# Plot horizontal line\n",
    "#plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "\n",
    "ax.plot(df.pos, df.instructions, \n",
    "             color=\"blue\", marker='x', label='Goal-oriented (persona)')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_title('Quality of responses to general instructions (↑)', fontproperties=bold_font, fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('Conversation round')\n",
    "ax.set_ylabel('Win rate')\n",
    "ax.grid(True)\n",
    "#plt.ylim(0, 1)\n",
    "# # Sanitize role name for filename\n",
    "# sanitized_role = role.replace('.json.csv', '').replace('.', '_').replace('/', '_')\n",
    "# filename = f'line_plot_{metric}_{sanitized_role}.png'\n",
    "# plt.savefig(filename)\n",
    "# plt.close() # Close the plot to free memory\n",
    "# print(f\"Generated plot: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a605c2-9db3-4b04-b2c1-2f3aef12d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/instruction_general.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c0f12-ce09-45a1-ae71-a0587838e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = ratings[~ratings.role.str.contains(\"empty\")][ratings.reference==\"dataset\"]\n",
    "# Calculate the difference (MAE_interview - MAE_instructions)\n",
    "filtered_df['rating_difference'] = filtered_df['interview'] - filtered_df['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140af01d-0ab2-47cd-87b9-cf23eb462764",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = filtered_df.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['rating_difference']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "df = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd013891-e88f-458a-9e7d-e588001065b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. general response quality difference (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/instruction_general_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538dfda2-8818-4426-a672-9c74b016626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()\n",
    "baseline_df = df[df['pos'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df1d72-d550-4e74-b463-46e59ed79790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd330b6a-bcb2-4bb4-bf59-d0a5eb55bf45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2bf5dd-4256-43c8-b559-8ce33dc4418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. general response quality improvement', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/instruction_general_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98dfb4-0fbc-45e4-9d04-65ec4e819ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings[(~ratings.role.str.contains(\"empty\"))  & (ratings.reference==\"dataset\")].copy()\n",
    "baseline_df =  ratings[(ratings.role.str.contains(\"empty\")) & (ratings.reference==\"dataset\")].copy()\n",
    "baseline_df.role = baseline_df.role.apply(lambda x: x.replace(\"empty-\", \"\"))\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\", \"pos\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6def03-856f-4d1e-9911-6b999a4c5057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc065b-cc6a-4131-b58a-e75cea6eaeff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc62804-5b02-4f5f-b18d-1712531b87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. general response quality difference (over baseline)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/instruction_general_difference_baseline.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b7eed-1bd6-4369-a17a-9322a6b61bc0",
   "metadata": {},
   "source": [
    "# Ifbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bbab2-b7c6-48e1-b5e3-b2a889335c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = json.load(open(\"./results/ifbench_results.json\", \"r\"))\n",
    "\n",
    "ratings = pd.DataFrame.from_dict(ratings).reset_index().melt(id_vars=['index'], var_name='model', value_name='rating').rename(columns=lambda x: \"role\" if x == \"index\" else x)\n",
    "\n",
    "data, _ = load_data(\"ifbench\")\n",
    "\n",
    "\n",
    "ratings[\"pos\"] = ratings.role.str.split(\"_\").str[-1].astype(int)\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.rsplit(\"_\", n=1).str[0]\n",
    "\n",
    "ratings[\"base_conversation\"] = ratings.role.apply(lambda x: \"instructions\" if \"inst-\" in x else \"interview\")\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.split(\"_\",n=1).str[-1].str.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1c3a2-a0d0-412b-a731-accb89497b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.groupby(['model', 'role', 'pos', \"base_conversation\"])['rating'].mean().unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac43a2-ba0b-4adf-adef-e42f73f652ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings[\"ifbench\"] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93a2a1-5954-4f7a-a6f3-bfa10772d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "df = ratings[(ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "ax.plot(df.pos, df.interview, \n",
    "             color=\"orange\", marker=\"o\", linestyle=\"--\", label='Persona-directed (baseline)')\n",
    "\n",
    "# Plot horizontal line\n",
    "#plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "\n",
    "ax.plot(df.pos, df.instructions, \n",
    "             color=\"blue\", marker='x',linestyle=\"--\", label='Goal-oriented (baseline)')\n",
    "\n",
    "df = ratings[(~ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "ax.plot(df.pos, df.interview, \n",
    "             color=\"orange\", marker='o', label='Persona-directed (persona)')\n",
    "\n",
    "# Plot horizontal line\n",
    "#plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "\n",
    "# Calculate mean and 95% CI for the dataset reference group\n",
    "\n",
    "ax.plot(df.pos, df.instructions, \n",
    "             color=\"blue\", marker='x', label='Goal-oriented (persona)')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_title('IFBench accuracy (↑)', fontproperties=bold_font, fontsize=14)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.5), ncol=2)\n",
    "ax.set_xlabel('Conversation round')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.grid(True)\n",
    "#plt.ylim(0, 1)\n",
    "# # Sanitize role name for filename\n",
    "# sanitized_role = role.replace('.json.csv', '').replace('.', '_').replace('/', '_')\n",
    "# filename = f'line_plot_{metric}_{sanitized_role}.png'\n",
    "# plt.savefig(filename)\n",
    "# plt.close() # Close the plot to free memory\n",
    "# print(f\"Generated plot: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c42390-991e-4f8b-a829-f8fd5f7b4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/ifbench.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb373f-f336-476f-96ea-35e0a8c5c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = ratings[~ratings.role.str.contains(\"empty\")]\n",
    "# Calculate the difference (MAE_interview - MAE_instructions)\n",
    "filtered_df['rating_difference'] = filtered_df['interview'] - filtered_df['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b2c04-9e2a-491d-8218-4d0f89d53dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = filtered_df.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['rating_difference']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "df = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd04429-2186-4892-ba9b-d69d6472fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. IfBench Acc. difference (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/ifbench_dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107c9e2-caa6-477b-914a-580930d9f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()\n",
    "baseline_df = df[df['pos'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9446254-bc55-497c-b3ce-ca0ef0fe7c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6de2a0-944f-4391-9a06-d0161bfa2caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0252f8e-f94e-4dff-be0e-9fa36f33a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. IfBench improvement', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/ifbench_degradation.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ea663-096c-4e30-8232-41154b562f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings[(~ratings.role.str.contains(\"empty\"))].copy()\n",
    "baseline_df =  ratings[(ratings.role.str.contains(\"empty\"))].copy()\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=[\"model\", \"pos\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429efb2-102a-40ff-bc49-28fcd3fb3c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ac65b-9a91-4843-9f39-d54e57efe21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged.groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd76b2-345f-4810-a6fd-a799cd81fbef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Avg. Difference', fontsize=12)\n",
    "plt.title('Avg. IfBench Acc. Difference (over baseline)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/ifbench_difference_baseline.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25375e-7081-4bde-861d-4d32663d8a57",
   "metadata": {},
   "source": [
    "# XStest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3495090-fc01-41f6-b84e-e8367f0c6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./results/xstest_ratings.csv\")\n",
    "\n",
    "data, _ = load_data(\"xstest\")\n",
    "\n",
    "labels = data[\"label\"]\n",
    "\n",
    "label_mapping = {\"safe\": 0, \"unsafe\": 1}\n",
    "labels = [label_mapping[x] for x in labels]\n",
    "\n",
    "rating_mapping = {\"No\": 0, \"Yes\": 1}\n",
    "ratings[\"rating\"] = ratings.rating.apply(lambda x: rating_mapping[x])\n",
    "\n",
    "ratings[\"correct\"] = ratings.apply(lambda x: x[\"rating\"] == labels[x[\"idx\"]],axis=1)\n",
    "\n",
    "ratings[\"pos\"] = ratings.role.str.extract(r\"(\\d+)(?=\\.json)\").astype(int)\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.rsplit(\"_\", n=1).str[0]\n",
    "\n",
    "ratings[\"base_conversation\"] = ratings.role.apply(lambda x: \"instructions\" if \"inst-\" in x else \"interview\")\n",
    "\n",
    "ratings[\"role\"] = ratings[\"role\"].str.split(\"_\",n=1).str[-1].str.replace(\"_\", \" \")\n",
    "\n",
    "ratings[\"label\"] = ratings.idx.apply(lambda x: labels[x])\n",
    "\n",
    "colors = {\n",
    "    'Refused safe': 'blue',\n",
    "    'Refused unsafe': 'red',\n",
    "    'Accuracy': 'green'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467c328-1136-4730-a196-68700540d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.groupby(['model', 'role', 'pos', \"label\", \"base_conversation\"])['rating'].mean().unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a808d0-67e2-4d83-a292-7ae9796d2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings[\"xstest\"] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858d54f-4ac3-47b7-ba38-d59e1ab9f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots, sharing the x-axis\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# --- Subplot 1: Refusal of unsafe queries (label==1) ---\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Data for 'no role'\n",
    "df1_no_role = ratings[(ratings.label == 1) &  (ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "ax1.plot(df1_no_role.pos, df1_no_role.interview, color=\"orange\", marker=\"o\", linestyle=\"--\", label='Persona-directed (baseline)')\n",
    "ax1.plot(df1_no_role.pos, df1_no_role.instructions, color=\"blue\", marker='x', linestyle=\"--\", label='Goal-oriented (baseline)')\n",
    "\n",
    "# Data for 'role'\n",
    "df1_role = ratings[(ratings.label == 1)  & (~ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "ax1.plot(df1_role.pos, df1_role.interview, color=\"orange\", marker='o', label='Persona-directed (persona)')\n",
    "ax1.plot(df1_role.pos, df1_role.instructions, color=\"blue\", marker='x', label='Goal-oriented (persona)')\n",
    "\n",
    "ax1.set_title('Refusal of unsafe queries (↑)', fontproperties=bold_font, fontsize=14)\n",
    "ax1.set_ylabel('Refusal rate')\n",
    "ax1.grid(True)\n",
    "#ax1.set_ylim(0.5, 1) # Set y-limit for unsafe queries\n",
    "\n",
    "# --- Subplot 2: Refusal of safe queries (label==0) ---\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Data for 'no role'\n",
    "df2_no_role = ratings[(ratings.label == 0) & (ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "ax2.plot(df2_no_role.pos, df2_no_role.interview, color=\"orange\", marker=\"o\", linestyle=\"--\", label='Persona-directed (baseline)')\n",
    "ax2.plot(df2_no_role.pos, df2_no_role.instructions, color=\"blue\", marker='x', linestyle=\"--\", label='Goal-oriented (baseline)')\n",
    "\n",
    "# Data for 'role'\n",
    "df2_role = ratings[(ratings.label == 0)  & (~ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "ax2.plot(df2_role.pos, df2_role.interview, color=\"orange\", marker='o', label='Persona-directed (persona)')\n",
    "ax2.plot(df2_role.pos, df2_role.instructions, color=\"blue\", marker='x', label='Goal-oriented (persona)')\n",
    "\n",
    "ax2.set_title('Refusal of safe queries (↓)', fontproperties=bold_font, fontsize=14)\n",
    "ax2.set_xlabel('Conversation round')\n",
    "ax2.set_ylabel('Refusal rate')\n",
    "ax2.grid(True)\n",
    "#ax2.set_ylim(0, 0.5) # Set y-limit for safe queries\n",
    "\n",
    "# --- Single Legend on Top in Custom Order ---\n",
    "all_handles, all_labels = ax1.get_legend_handles_labels()\n",
    "legend_dict = dict(zip(all_labels, all_handles))\n",
    "\n",
    "desired_order = [\n",
    "    'Persona-directed (baseline)',\n",
    "    'Persona-directed (persona)',\n",
    "    'Goal-oriented (baseline)',\n",
    "    'Goal-oriented (persona)'\n",
    "]\n",
    "\n",
    "ordered_handles = [legend_dict[label] for label in desired_order]\n",
    "ordered_labels = desired_order\n",
    "\n",
    "fig.legend(ordered_handles, ordered_labels, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.1))\n",
    "\n",
    "\n",
    "# Adjust layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust rect to make space for the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6fecec-5b9b-4745-9e4e-8f6897331cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/xstest.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6596531-214d-4ef5-a27a-d5ebc6ae8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = ratings[~ratings.role.str.contains(\"empty\")]\n",
    "# Calculate the difference (MAE_interview - MAE_instructions)\n",
    "filtered_df['rating_difference'] = filtered_df['interview'] - filtered_df['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e969f25-09a8-4aa3-9879-547a0f593898",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = filtered_df[filtered_df.label==0].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['rating_difference']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "df = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29567938-7a59-4270-8902-2b889793fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Refusal Rate Difference', fontsize=12)\n",
    "plt.title('Difference of Refusal of Safe Queries Rate (Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/xstest_safe_diff.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad700025-1f7a-49be-98ec-1a790522b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = filtered_df[filtered_df.label==1].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['rating_difference']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "df = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8443cc4-eaca-4080-904b-92d5feb12607",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(df['pos'], df['mean_difference'], color='magenta')\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(df['pos'], df['lower_bound_95_ci'], df['upper_bound_95_ci'], color='pink', alpha=0.4)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Refusal Rate Difference', fontsize=12)\n",
    "plt.title('Difference of Refusal of Unsafe Queries Rate(Persona-directed - Goal-oriented)', fontproperties=bold_font, fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/xstest_unsafe_diff.pdf', dpi=300, bbox_inches=\"tight\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ce6d2-f0be-4f86-8933-0d123496b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()\n",
    "baseline_df = df[df['pos'] == 0].set_index('role')\n",
    "\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=['role', \"model\", \"label\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21771a4f-c1a9-4f3e-a927-de7858176b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==0].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224c0e9-cba5-4b74-9b7d-59b457bf7acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==0].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0790f-1589-4e3d-92a7-c23c317de7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Refusal Rate Difference', fontsize=12)\n",
    "plt.title('Difference in Refusal Rate of Safe Queries (compared with round 0)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/xstest_safe_rate_change.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8db86a-7955-4827-8b9b-f7c24bcd76f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==1].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc442de0-9dfa-4d71-bd45-237404e1620b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==1].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a8370-5d2f-4ef8-88a3-b290688b14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Refusal Rate Difference', fontsize=12)\n",
    "plt.title('Difference in Refusal Rate of Unsafe Queries (compared with round 0)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/xstest_unsafe_rate_change.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d73bf3-01a3-49da-aff5-584981c630d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings[(~ratings.role.str.contains(\"empty\"))].copy()\n",
    "baseline_df =  ratings[(ratings.role.str.contains(\"empty\"))].copy()\n",
    "# Use merge to join the baseline values back to the original DataFrame\n",
    "df_merged = pd.merge(df, baseline_df, how=\"left\", on=[\"model\", \"pos\", \"label\"], suffixes=('', '_baseline'))\n",
    "\n",
    "# Calculate the difference for each column\n",
    "df_merged['persona_diff_from_idx_0'] = df_merged['interview'] - df_merged['interview_baseline']\n",
    "df_merged['goal_diff_from_idx_0'] = df_merged['instructions'] - df_merged['instructions_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edead245-e0d3-4357-a6bc-441a69867816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==0].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b736bb-46c4-4079-b509-0cc38d5fe152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==0].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb177f8-562a-43fa-9a68-833b565fa799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Refusal Rate Difference', fontsize=12)\n",
    "plt.title('Difference in Refusal Rate of Safe Queries (over baseline)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/xstest_safe_refusal_diff_baseline.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2671497-8dc1-41fa-b8e5-be3aebf0d860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==1].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['persona_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "persona_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec2ce7-519c-4660-b873-3c705926cfb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Create an empty list to store the results for each 'pos'\n",
    "results = []\n",
    "\n",
    "# Group the DataFrame by 'pos' and iterate through the groups\n",
    "grouped_pos = df_merged[df_merged.label==1].groupby('pos')\n",
    "\n",
    "for pos, group in grouped_pos:\n",
    "    # Get the 'MAE_difference' values for the current group\n",
    "    diffs = group['goal_diff_from_idx_0']\n",
    "\n",
    "    # Create an empty list to store the bootstrap means for this 'pos'\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_diffs = diffs.sample(n=len(diffs), replace=True)\n",
    "        bootstrap_means.append(resampled_diffs.mean())\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'pos': pos,\n",
    "        'mean_difference': np.mean(diffs),\n",
    "        'lower_bound_95_ci': lower_bound,\n",
    "        'upper_bound_95_ci': upper_bound\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "display((results_df[\"lower_bound_95_ci\"]*results_df[\"upper_bound_95_ci\"] > 0) & (results_df[\"mean_difference\"] <0)  )\n",
    "goal_df = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e92ab-40f9-4be5-b8b8-e87ec8ee5607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "df = results_df\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(persona_df['pos'], persona_df['mean_difference'], color='orange', label=\"Persona-directed\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(persona_df['pos'], persona_df['lower_bound_95_ci'], persona_df['upper_bound_95_ci'], color='orange', alpha=0.4)\n",
    "\n",
    "# Plot the mean differences as a line\n",
    "plt.plot(goal_df['pos'], goal_df['mean_difference'], color='blue', label=\"Goal Oriented\")\n",
    "\n",
    "# Fill the area for the 95% confidence interval\n",
    "plt.fill_between(goal_df['pos'], goal_df['lower_bound_95_ci'], goal_df['upper_bound_95_ci'], color='blue', alpha=0.4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Conversation round', fontsize=12)\n",
    "plt.ylabel('Refusal Rate Difference', fontsize=12)\n",
    "plt.title('Difference in Refusal Rate of Unsafe Queries (over baseline)', fontproperties=bold_font, fontsize=14)\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "plt.savefig('../persistent-personas-paper/media/xstest_unsafe_refusal_diff_baseline.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895dd13-d9e3-424f-964d-a418f86d8e93",
   "metadata": {},
   "source": [
    "### Regression plots (length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ade1ed-dd5b-456b-9343-1d40a1401d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0661cd-7fa3-4cd8-828c-b585ad6d416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./results/all_ratings.pkl\")\n",
    "if not path.exists():\n",
    "    pickle.dump(all_ratings, open(path, \"wb\"))\n",
    "else:\n",
    "    all_ratings = pickle.load( open(path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5465f-d03d-49c5-98aa-670f8f6c20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fecee-6ab0-4afe-9658-317efcff7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df = all_ratings[\"dialogue\"].groupby([x for x in all_ratings[\"dialogue\"] if x not in [\"metric\", \"persona\", \"goal\", \"diff\"]]).mean(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf8758-5fb7-428b-8964-b7f35ab409da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df = pd.melt(dialogue_df,\n",
    "                    id_vars=['model', 'role', 'idx', 'diff'],\n",
    "                    value_vars=['persona', 'goal'],\n",
    "                    var_name='original_column',\n",
    "                    value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c71c7-0401-4857-96c5-9e3993e18bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df.rating = (dialogue_df.rating - 1)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15263fee-b502-4f86-8f5e-e1459bf3cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df = dialogue_df.drop(columns=[\"diff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeca582-940a-45ea-9c3b-ab6ce334ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df = dialogue_df.rename(columns={\"idx\": \"pos\", \"original_column\": \"base_conversation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b5c6c-7af2-49cf-9060-402777ce2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df.base_conversation = dialogue_df.base_conversation.apply(lambda x: \"interview\" if x == \"persona\" else \"instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ded887-0e64-4c18-9b4d-c56e8bb4eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df.rating *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb41b8-b900-49e8-ab0d-54e82e2dbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df.sort_values([\"model\", \"role\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f3c96-ef3c-463c-8d40-50eefac93313",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfi_df = all_ratings[\"bfi\"][[\"model\", \"role\", \"pos\", \"base_conversation\", \"MAE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425ba40-5237-45fc-a066-b9b10576a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfi_df = bfi_df.rename(columns={\"MAE\": \"rating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfi_df.loc[~(bfi_df.role.str.contains(\"empty\")),\"rating\"] *=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec14a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfi_df.loc[(bfi_df.role.str.contains(\"empty\")),\"rating\"] = bfi_df[bfi_df.role.str.contains(\"empty\")][\"rating\"].apply(lambda x: {k:v*100 for k,v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9d7f4-6e70-4c99-ab32-e4a5dd1d9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_df = all_ratings[\"role_specific\"]\n",
    "role_df = role_df[role_df[\"reference\"]==\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffb72b-5244-4b5d-b118-d2186b38a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_df = role_df.drop(columns=\"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5992a6-a0f7-474d-b5eb-f907bc6afbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_df = pd.melt(role_df,\n",
    "                    id_vars=['model', 'role', 'pos'],\n",
    "                    value_vars=['instructions', 'interview'],\n",
    "                    var_name='base_conversation',\n",
    "                    value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c16c83-4207-42e7-a5fb-a5457c9d49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_df.rating*=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5e902-12d1-455d-ab79-244fb59a9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df = all_ratings[\"general\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c134b95-0a7e-4533-8ad5-0651ab2a16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df = general_df[general_df.reference==\"dataset\"].drop(columns=\"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495b186-e84e-4607-8090-98c400768604",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df = pd.melt(general_df,\n",
    "                    id_vars=['model', 'role', 'pos'],\n",
    "                    value_vars=['instructions', 'interview'],\n",
    "                    var_name='base_conversation',\n",
    "                    value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b785807-7474-4b0e-ab43-818c088e8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.rating *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8859b-1ab2-4dc6-974b-9da3525bccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifbench_df = all_ratings[\"ifbench\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcee04-86ba-4be3-a192-d43ac98c6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifbench_df = pd.melt(ifbench_df,\n",
    "                    id_vars=['model', 'role', 'pos'],\n",
    "                    value_vars=['instructions', 'interview'],\n",
    "                    var_name='base_conversation',\n",
    "                    value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b6cfa-431f-48c2-b739-7d73c04e9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifbench_df.rating *=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5a288-542c-4c36-a2cb-81e8602c1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "xstest_df = all_ratings[\"xstest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59197994-8da2-47ca-98b3-634b514eb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_df = pd.melt(xstest_df[xstest_df.label==1],\n",
    "                    id_vars=['model', 'role', 'pos'],\n",
    "                    value_vars=['instructions', 'interview'],\n",
    "                    var_name='base_conversation',\n",
    "                    value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1cc472-b78a-4320-aeaa-ce0f0e103e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_df.rating *=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912c440-b271-4b81-87a4-11fb9b8c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_df = pd.melt(xstest_df[xstest_df.label==0],\n",
    "                    id_vars=['model', 'role', 'pos'],\n",
    "                    value_vars=['instructions', 'interview'],\n",
    "                    var_name='base_conversation',\n",
    "                    value_name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262b3b8-75ed-4412-ad9d-1d4cea685dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_df.rating *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93480bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcode model order with Gemini as its own family at the end\n",
    "model_order = [\n",
    "\"gemma-3-4b-it\", \"gemma-3-27b-it\",\n",
    "\"Llama-3.1-Nemotron-Nano-8B-v1\", \"Llama-3_3-Nemotron-Super-49B-v1\",\n",
    "\"Qwen3-4B-Instruct-2507\", \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "\"gemini-2.5-flash\"\n",
    "]\n",
    "model_name_map = {\n",
    "\"Llama-3.1-Nemotron-Nano-8B-v1\": \"Nemotron-8B\",\n",
    "\"Llama-3_3-Nemotron-Super-49B-v1\": \"Nemotron-49B\",\n",
    "\"gemma-3-4b-it\": \"Gemma3-4B\",\n",
    "\"gemma-3-27b-it\": \"Gemma3-27B\",\n",
    "\"Qwen3-4B-Instruct-2507\": \"Qwen3-4B\",\n",
    "\"Qwen3-30B-A3B-Instruct-2507\": \"Qwen3-30B\",\n",
    "\"gemini-2.5-flash\": \"gemini-2.5-flash\"\n",
    "}\n",
    "model_size_map = {\n",
    "\"Llama-3.1-Nemotron-Nano-8B-v1\": 1,\n",
    "\"Llama-3_3-Nemotron-Super-49B-v1\": 2,\n",
    "\"gemma-3-4b-it\": 1,\n",
    "\"gemma-3-27b-it\": 2,\n",
    "\"Qwen3-4B-Instruct-2507\": 1,\n",
    "\"Qwen3-30B-A3B-Instruct-2507\": 2,\n",
    "\"gemini-2.5-flash\": 3\n",
    "}\n",
    "model_family_map = {\n",
    "\"Llama-3.1-Nemotron-Nano-8B-v1\": \"Nemotron\",\n",
    "\"Llama-3_3-Nemotron-Super-49B-v1\": \"Nemotron\",\n",
    "\"gemma-3-4b-it\": \"Gemma\",\n",
    "\"gemma-3-27b-it\": \"Gemma\",\n",
    "\"Qwen3-4B-Instruct-2507\": \"Qwen\",\n",
    "\"Qwen3-30B-A3B-Instruct-2507\": \"Qwen\",\n",
    "\"gemini-2.5-flash\": \"Gemini\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to bootstrap CI ---\n",
    "def bootstrap_ci(data, n_bootstrap=5000, ci=95, random_state=42):\n",
    "    \"\"\"\n",
    "    Bootstrap confidence intervals for the mean of data.\n",
    "    Returns mean, lower_ci, upper_ci.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    boot_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = rng.choice(data, size=len(data), replace=True)\n",
    "        boot_means.append(sample.mean())\n",
    "    mean = np.mean(boot_means)\n",
    "    lower = np.percentile(boot_means, (100 - ci) / 2)\n",
    "    upper = np.percentile(boot_means, 100 - (100 - ci) / 2)\n",
    "    return mean, lower, upper\n",
    "\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_with_bootstrap(\n",
    "    diff_dfs, model_name_map, bold_font, colors=None, split_by_base=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot bootstrap CIs of diff across multiple datasets.\n",
    "    \n",
    "    diff_dfs: dict of {dataset_name: dataframe}\n",
    "    model_name_map: dict mapping original model names to plot order\n",
    "    bold_font: matplotlib fontproperties\n",
    "    colors: list of bar colors (default two contrasting colors)\n",
    "    split_by_base: if True, create separate bars for each base_conversation\n",
    "    \"\"\"\n",
    "    nrows = len(diff_dfs)\n",
    "    figsize = (8, 3 * nrows)  # scale height by number of rows\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=figsize, sharex=True)\n",
    "\n",
    "    if nrows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    if colors is None:\n",
    "        colors = [\"#D3242D4B\", \"#A9A9A9\"]\n",
    "\n",
    "    # Map base_conversation labels\n",
    "    base_map = {\n",
    "        \"interview\": \"Persona-directed\",\n",
    "        \"instructions\": \"Goal-oriented\",\n",
    "    }\n",
    "    base_order = [\"Persona-directed\", \"Goal-oriented\"]\n",
    "\n",
    "    for idx, (dataset, df) in enumerate(diff_dfs.items()):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        results = []\n",
    "\n",
    "        if split_by_base:\n",
    "            # Relabel base_conversation\n",
    "            df = df.copy()\n",
    "            df[\"base_conversation\"] = df[\"base_conversation\"].map(base_map)\n",
    "\n",
    "            # group by model + base_conversation\n",
    "            for (model, base), group in df.groupby([\"model\", \"base_conversation\"]):\n",
    "                mean, low, high = bootstrap_ci(group[\"diff\"].values)\n",
    "                results.append({\n",
    "                    \"model\": model,\n",
    "                    \"base_conversation\": base,\n",
    "                    \"mean\": mean,\n",
    "                    \"low\": low,\n",
    "                    \"high\": high\n",
    "                })\n",
    "            df_boot = pd.DataFrame(results)\n",
    "\n",
    "            # Ensure consistent ordering of models + bases\n",
    "            df_boot[\"model\"] = pd.Categorical(\n",
    "                df_boot[\"model\"], categories=list(model_name_map.values()), ordered=True\n",
    "            )\n",
    "            df_boot[\"base_conversation\"] = pd.Categorical(\n",
    "                df_boot[\"base_conversation\"], categories=base_order, ordered=True\n",
    "            )\n",
    "            df_boot = df_boot.sort_values([\"model\", \"base_conversation\"])\n",
    "\n",
    "            # Pivot so each base conversation is a separate column\n",
    "            df_plot = df_boot.pivot(index=\"model\", columns=\"base_conversation\", values=\"mean\")\n",
    "\n",
    "            # Plot grouped bars\n",
    "            df_plot.plot(\n",
    "                kind=\"bar\",\n",
    "                ax=ax,\n",
    "                color=colors[:len(df_plot.columns)],\n",
    "                width=0.8,\n",
    "                legend=False  # suppress auto-legend here\n",
    "            )\n",
    "\n",
    "            # Add error bars manually\n",
    "            n_bases = len(df_plot.columns)\n",
    "            for i, model in enumerate(df_plot.index):\n",
    "                for j, base in enumerate(df_plot.columns):\n",
    "                    row = df_boot[(df_boot[\"model\"] == model) & (df_boot[\"base_conversation\"] == base)].iloc[0]\n",
    "                    ax.errorbar(\n",
    "                        x=i + (j - (n_bases-1)/2) * (0.8 / n_bases),\n",
    "                        y=row[\"mean\"],\n",
    "                        yerr=[[row[\"mean\"] - row[\"low\"]], [row[\"high\"] - row[\"mean\"]]],\n",
    "                        fmt=\"none\",\n",
    "                        ecolor=\"black\",\n",
    "                        capsize=4,\n",
    "                        elinewidth=1,\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            # group by model only\n",
    "            for model, group in df.groupby(\"model\"):\n",
    "                mean, low, high = bootstrap_ci(group[\"diff\"].values)\n",
    "                results.append({\"model\": model, \"mean\": mean, \"low\": low, \"high\": high})\n",
    "\n",
    "            df_boot = pd.DataFrame(results).set_index(\"model\")\n",
    "            df_boot = df_boot.reindex(list(model_name_map.values()))\n",
    "\n",
    "            bars = ax.bar(df_boot.index, df_boot[\"mean\"], color=colors[0], width=0.8)\n",
    "\n",
    "            # Add error bars\n",
    "            ax.errorbar(\n",
    "                x=np.arange(len(df_boot.index)),\n",
    "                y=df_boot[\"mean\"],\n",
    "                yerr=[df_boot[\"mean\"] - df_boot[\"low\"], df_boot[\"high\"] - df_boot[\"mean\"]],\n",
    "                fmt=\"none\", ecolor=\"black\", capsize=4, elinewidth=1,\n",
    "            )\n",
    "\n",
    "        # --- Subplot Customization ---\n",
    "        ax.set_title(dataset.capitalize(), fontproperties=bold_font, fontsize=14, pad=10)\n",
    "        ax.set_ylabel(\"Avg. Diff\", fontsize=12)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "    # Bottom subplot x-labels\n",
    "    bottom_ax = axes[-1]\n",
    "    bottom_ax.tick_params(axis=\"x\", rotation=45, labelsize=11)\n",
    "    bottom_ax.set_xlabel(\"Model\")\n",
    "\n",
    "    # Add a single shared legend on top if splitting by base\n",
    "    if split_by_base:\n",
    "        fig.legend(\n",
    "            labels=base_order,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, .915),\n",
    "            ncol=len(base_order),\n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "        )\n",
    "\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_dfs = {}\n",
    "for df, dataset in  zip([dialogue_df, bfi_df, role_df, general_df, ifbench_df, safety_df, excess_df], [\"dialogue\", \"bfi\", \"role\", \"general\", \"ifbench\", \"safety\", \"excess\"]):\n",
    "    df = df.copy()\n",
    "    df = df[~df.role.str.contains(\"empty\")]\n",
    "    df[\"size\"] = df.model.apply(lambda x: model_size_map[x])\n",
    "    df[\"family\"] = df.model.apply(lambda x: model_family_map[x])\n",
    "    df[\"model\"] = df.model.apply(lambda x: model_name_map[x])\n",
    "    df[\"roleFamily\"] = df.role + df.family\n",
    "    positions = np.unique(df.pos.sort_values())\n",
    "    df_first = df[df.pos==positions[0]].copy()\n",
    "    df_last = df[df.pos==positions[-1]].copy()\n",
    "    df_last[\"diff\"] = (df_last.rating.values - df_first.rating.values).astype(\"float\")\n",
    "    md = smf.mixedlm(\"diff ~ size\", df_last, groups=df_last[\"roleFamily\"]) \n",
    "    mdf = md.fit(method=[\"powell\", \"lbfgs\"])\n",
    "    coefs = mdf.summary().tables[1]\n",
    "    coefs[\"Coef.\"] = coefs[\"Coef.\"].astype(\"float\")\n",
    "    display(dataset)\n",
    "    display(coefs)\n",
    "    diff_dfs[dataset] = df_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544afdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_with_bootstrap(diff_dfs, model_name_map, bold_font, split_by_base=True, colors=[\"orange\", \"blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/length_cost.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_dfs = {}\n",
    "for df, dataset in  zip([general_df, ifbench_df, safety_df, excess_df], [\"general\", \"ifbench\", \"safety\", \"excess\"]):\n",
    "    df = df.copy()\n",
    "    df = df[df.pos==0]\n",
    "    df[\"size\"] = df.model.apply(lambda x: model_size_map[x])\n",
    "    df[\"family\"] = df.model.apply(lambda x: model_family_map[x])\n",
    "    df[\"model\"] = df.model.apply(lambda x: model_name_map[x])\n",
    "    df[\"roleFamily\"] = df.role + df.family\n",
    "    positions = np.unique(df.pos.sort_values())\n",
    "    df_roles = df[~df.role.str.contains(\"empty\")].copy()\n",
    "    df_no_roles = df[df.role.str.contains(\"empty\")].copy()\n",
    "    df_merge = pd.merge(df_roles, df_no_roles, how=\"left\", on=[\"model\", \"pos\", \"base_conversation\"], suffixes=(\"\", \"_no_role\"))\n",
    "    df_merge[\"diff\"] = (df_merge.rating.values - df_merge.rating_no_role.values).astype(\"float\")\n",
    "    md = smf.mixedlm(\"diff ~ size\", df_merge, groups=df_merge[\"roleFamily\"]) \n",
    "    mdf = md.fit(method=[\"powell\", \"lbfgs\"])\n",
    "    coefs = mdf.summary().tables[1]\n",
    "    coefs[\"Coef.\"] = coefs[\"Coef.\"].astype(\"float\")\n",
    "    display(dataset)\n",
    "    display(coefs)\n",
    "    diff_dfs[dataset] = df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4805e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_with_bootstrap(diff_dfs, model_name_map, bold_font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41150e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/personalization_cost.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5854b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_dfs = {}\n",
    "for df, dataset in  zip([dialogue_df, bfi_df, role_df, general_df, ifbench_df, safety_df, excess_df], [\"dialogue\", \"bfi\", \"role\", \"general\", \"ifbench\", \"safety\", \"excess\"]):\n",
    "    df = df.copy()\n",
    "    df = df[~df.role.str.contains(\"empty\")]\n",
    "    df[\"size\"] = df.model.apply(lambda x: model_size_map[x])\n",
    "    df[\"family\"] = df.model.apply(lambda x: model_family_map[x])\n",
    "    df[\"model\"] = df.model.apply(lambda x: model_name_map[x])\n",
    "    df_persona = df[df.base_conversation == \"interview\"].copy()\n",
    "    df_goal = df[df.base_conversation == \"instructions\"].copy()\n",
    "    df_merge = pd.merge(df_persona, df_goal, how=\"left\", on=[\"model\", \"pos\", \"role\"], suffixes=(\"\", \"_goal\"))\n",
    "    df_merge[\"diff\"] = (df_merge.rating.values - df_merge.rating_goal.values).astype(\"float\")\n",
    "    diff_dfs[dataset] = df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_with_bootstrap(diff_dfs, model_name_map, bold_font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ee6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/dialogue_diff.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d778fe9-901a-4d29-ad5d-91392d26ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs = [dialogue_df, bfi_df, role_df, general_df, ifbench_df, safety_df, excess_df]\n",
    "datasets = [\"Dialogue metrics\", \"BFI error\", \"Persona Instructions Quality\", \"General Instructions Quality\", \"IFBench Acc.\", \"Safety\", \"Excess Safety\"]\n",
    "\n",
    "# Create the figure and subplots with 7 rows and 7 columns\n",
    "fig, axes = plt.subplots(nrows=7, ncols=7, figsize=(21, 21), sharex=True)\n",
    "\n",
    "# Loop through models for rows\n",
    "for row_idx, model in enumerate(model_order):\n",
    "    # Loop through datasets for columns\n",
    "    for col_idx, (data_df, dataset) in enumerate(zip(data_dfs, datasets)):\n",
    "        data_df = data_df.copy()\n",
    "        if dataset == \"BFI error\":\n",
    "            data_df.loc[data_df.role.str.contains(\"empty\"), \"rating\"] = data_df[data_df.role.str.contains(\"empty\")][\"rating\"].apply(lambda x: np.mean(list(x.values())))\n",
    "        ratings = data_df.groupby([\"model\", \"pos\",\"role\", \"base_conversation\"])['rating'].mean().unstack(fill_value=0).reset_index().copy()\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Filter data for the specific model and remove empty roles\n",
    "        df = ratings[(ratings.model == model)  & (~ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "        # Plot for 'interview' (Persona-directed)\n",
    "        ax.plot(df.pos, df.interview, \n",
    "                     color=\"orange\", marker='o', label='Persona-directed')\n",
    "        \n",
    "\n",
    "        # Plot horizontal line\n",
    "        #plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "        \n",
    "        # Calculate mean and 95% CI for the dataset reference group\n",
    "        \n",
    "        ax.plot(df.pos, df.instructions, \n",
    "                     color=\"blue\", marker='x', label='Goal-oriented')\n",
    "        df = ratings[(ratings.model == model)  & (ratings.role.str.contains(\"empty\"))].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "        # Plot for 'instructions' (Goal-oriented)\n",
    "        ax.plot(df.pos, df.interview, \n",
    "                 color=\"orange\", marker='o', linestyle='--', label='Persona-directed (baseline)')\n",
    "        ax.plot(df.pos, df.instructions, \n",
    "                 color=\"blue\", marker='x', linestyle='--', label='Goal-oriented (baseline)')\n",
    "        \n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Set titles and labels for the grid\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(dataset, fontproperties=bold_font, fontsize=14, pad=20)\n",
    "        \n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f'{model_name_map[model]}', fontproperties=bold_font, fontsize=14, rotation=0, ha='right')\n",
    "            ax.yaxis.set_label_coords(-0.2, 0.5)\n",
    "\n",
    "        if row_idx == 6:\n",
    "            ax.set_xlabel('Conversation round', fontsize=12)\n",
    "\n",
    "# Get handles and labels from a sample plot to create the figure-level legend\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, .99), ncol=2, fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaee621-c6ad-45c2-a6e2-cc8869cf9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/results_per_model.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fea7a6-59b4-4faf-b1c5-a5d253ebc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df.role = dialogue_df.role.str.replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea27f0e-dc2d-49ce-ab2a-7af104b11ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = [x for x in np.unique(general_df.role) if \"empty\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368874f7-638e-451c-913f-5df2a4777731",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs = [dialogue_df, bfi_df, role_df, general_df, ifbench_df, safety_df, excess_df]\n",
    "datasets = [\"Dialogue metrics\", \"BFI error\", \"Persona Instructions Quality\", \"General Instructions Quality\", \"IFBench Acc.\", \"Safety\", \"Excess Safety\"]\n",
    "\n",
    "# Create the figure and subplots with 7 rows and 7 columns\n",
    "fig, axes = plt.subplots(nrows=8, ncols=7, figsize=(24, 21), sharex=True)\n",
    "\n",
    "# Loop through models for rows\n",
    "for row_idx, role in enumerate(roles):\n",
    "    # Loop through datasets for columns\n",
    "    for col_idx, (data_df, dataset) in enumerate(zip(data_dfs, datasets)):\n",
    "        if role not in data_df.role.values:\n",
    "            continue\n",
    "        data_df = data_df.copy()\n",
    "        if dataset == \"BFI error\":\n",
    "            data_df.loc[data_df.role.str.contains(\"empty\"), \"rating\"] = data_df[data_df.role.str.contains(\"empty\")][\"rating\"].apply(lambda x: x[role])\n",
    "        ratings = data_df.groupby([\"pos\",\"role\", \"base_conversation\"])['rating'].mean().unstack(fill_value=0).reset_index().copy()\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        # Filter data for the specific model and remove empty roles\n",
    "        df = ratings[(ratings.role == role)].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "        # Plot for 'interview' (Persona-directed)\n",
    "        ax.plot(df.pos, df.interview, \n",
    "                     color=\"orange\", marker='o', label='Persona-directed')\n",
    "        \n",
    "        # Plot horizontal line\n",
    "        #plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "        \n",
    "        # Calculate mean and 95% CI for the dataset reference group\n",
    "        \n",
    "        ax.plot(df.pos, df.instructions, \n",
    "                     color=\"blue\", marker='x', label='Goal-oriented')\n",
    "        \n",
    "        df = ratings[ratings.role.str.contains(\"empty\")].groupby(['pos']).mean(numeric_only=True).reset_index()\n",
    "        # Plot for 'interview' (Persona-directed)\n",
    "        ax.plot(df.pos, df.interview, \n",
    "                     color=\"orange\", marker='o', label='Persona-directed (baseline)', linestyle='--')\n",
    "        ax.plot(df.pos, df.instructions, \n",
    "                     color=\"blue\", marker='x', label='Goal-oriented (baseline)', linestyle='--')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Set titles and labels for the grid\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(dataset, fontproperties=bold_font, fontsize=14, pad=20)\n",
    "        \n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f'{role}', fontproperties=bold_font, fontsize=14, rotation=0, ha='right')\n",
    "            ax.yaxis.set_label_coords(-0.2, 0.5)\n",
    "\n",
    "        if row_idx == 6:\n",
    "            ax.set_xlabel('Conversation round', fontsize=12)\n",
    "\n",
    "# Get handles and labels from a sample plot to create the figure-level legend\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, .99), ncol=2, fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbde17-0bfb-441d-bbc6-d2571fbb149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/results_per_role.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc09eaa-14c4-457e-9631-85d6a2e1d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    \"Persona-directed\": \"orange\",\n",
    "    \"Goal-oriented\": \"blue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b69740-a197-4644-8167-69671c108d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(16, 12), sharex=True)\n",
    "axes = axes.flatten()  # Flatten the 4x2 array of axes into a 1D array for easier iteration\n",
    "\n",
    "# The first subplot (axes[0]) will be used for the legend, so the loop starts at index 1\n",
    "for i, (data_df, dataset) in enumerate(zip([dialogue_df, bfi_df, role_df, general_df, ifbench_df, safety_df, excess_df],\n",
    "                                           [\"Dialogue metrics\", \"BFI error\", \"Role Specific Instructions Quality\", \n",
    "                                            \"General Instructions Quality\", \"IFBench Accuracy\", \"Safety\", \"Excess Safety\"])):\n",
    "    \n",
    "    # Select the current subplot axis\n",
    "    ax = axes[i + 1]\n",
    "    \n",
    "    data_df = data_df.copy()\n",
    "    coeffs = {}\n",
    "    \n",
    "    # Check if there's enough data to perform the mixed model analysis\n",
    "    if len(np.unique(data_df.model)) > 1 and len(np.unique(data_df.base_conversation)) > 1:\n",
    "        for model in np.unique(data_df.model):\n",
    "            for dialogue in [\"interview\", \"instructions\"]:\n",
    "                df = data_df[(data_df.model == model) & (data_df.base_conversation == dialogue) & (~data_df.role.str.contains(\"empty\"))].copy()\n",
    "                \n",
    "                # Check if the dataframe is not empty before running the model\n",
    "                if not df.empty and len(df['role'].unique()) > 1:\n",
    "                    df.pos = df.pos.astype(\"int\")\n",
    "                    df.rating = df.rating.astype(\"float\")\n",
    "                    md = smf.mixedlm(\"rating ~ pos\", df, groups=df[\"role\"]) \n",
    "                    try:\n",
    "                        mdf = md.fit(method=[\"powell\", \"lbfgs\"])\n",
    "                        coefs = mdf.summary().tables[1]\n",
    "                        if \"pos\" in coefs.index:\n",
    "                            coefs[\"Coef.\"] = coefs[\"Coef.\"].astype(\"float\")\n",
    "                            coeffs[(model, dialogue)] = coefs.loc[\"pos\"]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not fit mixed model for {model} and {dialogue}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "    if not coeffs:\n",
    "        print(f\"Skipping plot for {dataset} due to insufficient data for mixed model analysis.\")\n",
    "        ax.set_title(f\"No Data for {dataset}\", fontsize=14)\n",
    "        ax.axis('off') # Hide the empty subplot\n",
    "        continue\n",
    "    \n",
    "    df = pd.DataFrame(coeffs).T.reset_index()\n",
    "    df = df.rename(columns={\"level_0\": \"model\", \"level_1\": \"base_conversation\"})\n",
    "    \n",
    "    numeric_cols = [\"Coef.\", \"Std.Err.\", \"z\", \"P>|z|\", \"[0.025\", \"0.975]\"]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    \n",
    "    df[\"model\"] = pd.Categorical(df[\"model\"], categories=model_order, ordered=True)\n",
    "    df = df.sort_values(\"model\")\n",
    "    \n",
    "    # Color scheme\n",
    "    color_map = {\"interview\": \"orange\", \"instructions\": \"blue\"}\n",
    "    \n",
    "    # X positions for dodge\n",
    "    x_vals = np.arange(len(model_order))\n",
    "    offset = 0.15  # displacement between dialogue modes\n",
    "    \n",
    "    for j, mode in enumerate(df[\"base_conversation\"].unique()):\n",
    "        subdf = df[df[\"base_conversation\"] == mode]\n",
    "        # align to integer positions with displacement\n",
    "        x = x_vals + (j - 0.5) * 2 * offset  \n",
    "        y = subdf[\"Coef.\"]\n",
    "        lower = subdf[\"[0.025\"]\n",
    "        upper = subdf[\"0.975]\"]\n",
    "        \n",
    "        ax.errorbar(\n",
    "            x, y,\n",
    "            yerr=[y - lower, upper - y],\n",
    "            fmt=\"o\", capsize=4, color=color_map[mode],\n",
    "            label=mode, alpha=0.9\n",
    "        )\n",
    "    \n",
    "    # Reference line\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    \n",
    "    # Family separators\n",
    "    family_boundaries = [1.5, 3.5, 5.5]\n",
    "    for xb in family_boundaries:\n",
    "        ax.axvline(x=xb, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(\"Model\", fontsize=10)\n",
    "    ax.set_ylabel(\"Coefficient\", fontsize=10)\n",
    "    ax.set_title(f\"Impact of length on {dataset}\", fontproperties=bold_font, fontsize=12)\n",
    "    \n",
    "    # Grid style\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    \n",
    "    # Use model names as ticks centered\n",
    "    renamed_labels = [model_name_map.get(m, m) for m in model_order]\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.set_xticklabels(renamed_labels, rotation=45, ha=\"right\")\n",
    "    \n",
    "# Create a proxy artist for the legend\n",
    "handles, labels = axes[1].get_legend_handles_labels() # get handles from a populated subplot\n",
    "legend_ax = axes[0] # The first subplot for the legend\n",
    "legend_ax.legend(handles, [\"Persona-directed\", \"Goal-oriented\"], loc='center', fontsize=16)\n",
    "legend_ax.axis('off') # Hide the axes and frame of the legend subplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6482e7-6a58-4088-8aff-c62b2e493790",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/length_coeffs.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02cda95-c838-4bfe-83c2-a7d96fe81b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 6), sharex=True)\n",
    "axes = axes.flatten()  # Flatten the 4x2 array of axes into a 1D array for easier iteration\n",
    "for i, (data_df, dataset) in enumerate(zip([general_df, ifbench_df, safety_df, excess_df],\n",
    "                                           [\"General Instruction Quality\", \"IFBench\", \"Safety\", \"Excess Safety\"])):\n",
    "    \n",
    "    ax = axes[i] # Use the current index\n",
    "\n",
    "        \n",
    "    data_df = data_df.copy()\n",
    "    coeffs = {}\n",
    "    \n",
    "    # Check if enough data exists to perform the mixed model analysis\n",
    "    if len(np.unique(data_df.model)) > 1 and len(np.unique(data_df.base_conversation)) > 1:\n",
    "        for model in np.unique(data_df.model):\n",
    "            for dialogue in [\"interview\", \"instructions\"]:\n",
    "                df = data_df[(data_df.model == model) & (data_df.base_conversation == dialogue)].copy()\n",
    "                df[\"group\"] = df.role.apply(lambda x: \"persona\" if \"empty\" not in x else \"empty\")\n",
    "                df[\"posRole\"] = df[\"pos\"].astype(str) + df[\"role\"].astype(str)\n",
    "                \n",
    "                # Check for a valid dataset before running the model\n",
    "                if not df.empty and len(df['posRole'].unique()) > 1 and \"empty\" in df['group'].values and \"persona\" in df['group'].values:\n",
    "                    try:\n",
    "                        md = smf.mixedlm(\"rating ~ C(group, Treatment(reference='empty'))\", df, groups=df[\"posRole\"])\n",
    "                        mdf = md.fit(method=[\"powell\", \"lbfgs\"])\n",
    "                        coefs = mdf.summary().tables[1]\n",
    "                        if \"C(group, Treatment(reference='empty'))[T.persona]\" in coefs.index:\n",
    "                            coefs[\"Coef.\"] = pd.to_numeric(coefs[\"Coef.\"], errors='coerce')\n",
    "                            coeffs[(model, dialogue)] = coefs.loc[\"C(group, Treatment(reference='empty'))[T.persona]\"]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not fit mixed model for {model} and {dialogue}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "    if not coeffs:\n",
    "        print(f\"Skipping plot for {dataset} due to insufficient data for mixed model analysis.\")\n",
    "        ax.set_title(f\"No Data for {dataset}\", fontproperties=bold_font, fontsize=12)\n",
    "        ax.axis('off') # Hide the empty subplot\n",
    "        continue\n",
    "    \n",
    "    df = pd.DataFrame(coeffs).T.reset_index()\n",
    "    df = df.rename(columns={\"level_0\": \"model\", \"level_1\": \"base_conversation\"})\n",
    "    \n",
    "    numeric_cols = [\"Coef.\", \"Std.Err.\", \"z\", \"P>|z|\", \"[0.025\", \"0.975]\"]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    \n",
    "    df[\"model\"] = pd.Categorical(df[\"model\"], categories=model_order, ordered=True)\n",
    "    df = df.sort_values(\"model\")\n",
    "    \n",
    "    # Color scheme\n",
    "    color_map = {\"interview\": \"orange\", \"instructions\": \"blue\"}\n",
    "    \n",
    "    # X positions for dodge\n",
    "    x_vals = np.arange(len(model_order))\n",
    "    offset = 0.15  # displacement between dialogue modes\n",
    "    \n",
    "    for j, mode in enumerate(df[\"base_conversation\"].unique()):\n",
    "        subdf = df[df[\"base_conversation\"] == mode]\n",
    "        \n",
    "        # Filter out models not present in the subdf\n",
    "        present_models_in_order = [m for m in model_order if m in subdf['model'].values]\n",
    "        \n",
    "        # Re-index the x_vals to match the present models\n",
    "        present_x_vals = [x_vals[model_order.index(m)] for m in present_models_in_order]\n",
    "        \n",
    "        x = np.array(present_x_vals) + (j - 0.5) * 2 * offset  \n",
    "        y = subdf.set_index('model').loc[present_models_in_order][\"Coef.\"]\n",
    "        lower = subdf.set_index('model').loc[present_models_in_order][\"[0.025\"]\n",
    "        upper = subdf.set_index('model').loc[present_models_in_order][\"0.975]\"]\n",
    "        \n",
    "        ax.errorbar(\n",
    "            x, y,\n",
    "            yerr=[y - lower, upper - y],\n",
    "            fmt=\"o\", capsize=4, color=color_map[mode],\n",
    "            label=mode, alpha=0.9\n",
    "        )\n",
    "    \n",
    "    # Reference line\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    \n",
    "    # Family separators\n",
    "    family_boundaries = [1.5, 3.5, 5.5]\n",
    "    for xb in family_boundaries:\n",
    "        ax.axvline(x=xb, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(\"Model\", fontsize=10)\n",
    "    ax.set_ylabel(\"Coefficient\", fontsize=10)\n",
    "    ax.set_title(f\"Impact of personalization on {dataset}\", fontproperties=bold_font, fontsize=12)\n",
    "    \n",
    "    # Grid style\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    \n",
    "    # Use model names as ticks centered\n",
    "    renamed_labels = [model_name_map.get(m, m) for m in model_order]\n",
    "    ax.set_xticks(x_vals)\n",
    "    ax.set_xticklabels(renamed_labels, rotation=45, ha=\"right\")\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "# Place the legend at the top of the entire figure\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.02), ncol=2, fontsize=12)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902219b-9f41-471f-983e-12cc196c34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/persona_coeffs.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f745dd1-e3fe-46de-9a32-cccd7214fe4e",
   "metadata": {},
   "source": [
    "### Control for Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4e945-f015-445e-8737-fa4ae0eeca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.read_csv(\"results/round_lengths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdedb4a-08b7-48f8-82f3-9c017e11f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6))\n",
    "axes = axes.flatten()  # Flatten the 2x4 array of axes into a 1D array for easier iteration\n",
    "y_labels = [\"Avg. Rating\", \"Mean Absolute Difference\", \"Win Rate\", \"Win Rate\", \"Acc.\", \"Refusal Rate\", \"Refusal Rate\"]\n",
    "# The first subplot (axes[0]) will be used for the legend\n",
    "# The loop will start from the second subplot (index 1)\n",
    "for i, (df, dataset) in enumerate(zip([dialogue_df, bfi_df, role_df, general_df, ifbench_df, safety_df, excess_df],\n",
    "                                           [\"Dialogue metrics\", \"BFI error\", \"Persona Instructions Quality\", \"General Instructions Quality\", \"IFBench Acc.\", \"Safety\", \"Excess Safety\"])):\n",
    "\n",
    "    # Select the current subplot axis, starting from the second one (i+1)\n",
    "    ax = axes[i + 1]\n",
    "\n",
    "    df = df[~df.role.str.contains(\"empty\")].copy()\n",
    "    df[\"role\"] = df.role.str.replace(\"_\", \" \")\n",
    "    \n",
    "    persona_df = df[df.base_conversation == \"interview\"].copy()\n",
    "    goal_df = df[df.base_conversation == \"instructions\"].copy()\n",
    "    \n",
    "    # Merge with lengths dataframe\n",
    "    if not lengths.empty:\n",
    "        persona_df = pd.merge(persona_df, lengths[[x for x in lengths.columns if \"goal\" not in x]], on=[\"model\", \"role\", \"pos\"], how='left')\n",
    "        persona_df = persona_df.rename(columns=lambda x: x.replace(\"_persona_directed\", \"\"))\n",
    "        \n",
    "        goal_df = pd.merge(goal_df, lengths[[x for x in lengths.columns if \"persona\" not in x]], on=[\"model\", \"role\", \"pos\"], how='left')\n",
    "        goal_df = goal_df.rename(columns=lambda x: x.replace(\"_goal_oriented\", \"\"))\n",
    "    \n",
    "    df = pd.concat([persona_df, goal_df])\n",
    "    \n",
    "    # Check if df is empty before proceeding\n",
    "    if df.empty or 'n_tokens' not in df.columns:\n",
    "        ax.set_title(f\"No Data for {dataset}\", fontproperties=bold_font, fontsize=12)\n",
    "        ax.axis('off') # Hide the empty subplot\n",
    "        continue\n",
    "    data_df = df.copy()\n",
    "    data_df.rating = data_df.rating.astype(\"float\")\n",
    "    # Group and plot for \"interview\" (Persona-directed)\n",
    "    ratings = data_df[~data_df.role.str.contains(\"empty\")].groupby([\"pos\", \"base_conversation\"])[['rating', \"n_tokens\"]].mean().reset_index().copy()\n",
    "    df = ratings[(ratings.base_conversation==\"interview\") ].groupby(['n_tokens']).mean(numeric_only=True).reset_index()\n",
    "    ax.plot(df.n_tokens, df.rating, \n",
    "                     color=\"orange\", marker='o', label='Persona-directed')\n",
    "        \n",
    "    # Plot horizontal line\n",
    "    #plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "    \n",
    "    # Calculate mean and 95% CI for the dataset reference group\n",
    "    df = ratings[(ratings.base_conversation==\"instructions\") ].groupby(['n_tokens']).mean(numeric_only=True).reset_index()\n",
    "    \n",
    "    ax.plot(df.n_tokens, df.rating, \n",
    "                 color=\"blue\", marker='x', label='Goal-oriented')\n",
    "    \n",
    "    # Set titles and labels for each subplot\n",
    "    ax.set_title(f'{dataset}', fontproperties=bold_font, fontsize=12)\n",
    "    ax.set_ylabel(y_labels[i], fontsize=12)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Only set x-labels for the bottom row of plots\n",
    "    if i >= 3:\n",
    "        ax.set_xlabel('Dialogue length (# tokens)', fontsize=10)\n",
    "\n",
    "# Create a proxy artist for the legend in the first subplot\n",
    "legend_ax = axes[0]\n",
    "legend_ax.plot([], [], 'o', color='orange', label='Persona-directed')\n",
    "legend_ax.plot([], [], 'x', color='blue', label='Goal-oriented')\n",
    "legend_ax.legend(loc='center', fontsize=16)\n",
    "legend_ax.axis('off') # Hide the axes and frame of the legend subplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7abf4b-6a59-4e70-907d-0c634a006491",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../persistent-personas-paper/media/length_control.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd99e70-2f88-4bc9-9c4f-7732030922c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persistent-personas",
   "language": "python",
   "name": "persistent-personas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
